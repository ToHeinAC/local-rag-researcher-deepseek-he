{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ollama\n",
    "import ollama\n",
    "import re\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "        {\n",
    "            \"content\": \"DeepSeek R1 is a reasoning-focused large language model (LLM) that leverages pure reinforcement learning to develop advanced reasoning abilities. It's optimized for complex reasoning tasks and delivers exceptional performance on logic, mathematics, and reasoning benchmarks while maintaining strong general capabilities.\",\n",
    "            \"metadata\": {\n",
    "                \"name\": \"DeepSeek R1 Documentation\",\n",
    "                \"path\": \"/data/documents/deepseek_r1_docs.pdf\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Retrieval-Augmented Generation (RAG) enhances LLM responses by retrieving relevant context before generating answers. This approach grounds responses in factual information, reducing hallucinations. When using DeepSeek R1 for RAG applications, leverage R1's reasoning capabilities in the generation stage, not for retrieval.\",\n",
    "            \"metadata\": {\n",
    "                \"name\": \"RAG Best Practices Guide\",\n",
    "                \"path\": \"/data/knowledge/rag_best_practices.md\"\n",
    "            }\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_summarizer(context_documents, llm_model=\"deepseek-r1\"):\n",
    "    system_message = \"\"\"\n",
    "    You are an expert summarizer working within a RAG system. Your task is to create a concise, accurate summary of the provided information while properly attributing all facts to their sources.\n",
    "\n",
    "    Guidelines:\n",
    "    - Create a clear, coherent summary limited to 3-5 sentences\n",
    "    - Focus on the most important facts and insights\n",
    "    - Maintain factual accuracy without adding new information\n",
    "    - Use neutral, professional language\n",
    "    - Cite EVERY piece of information using the format [Document Name](document_path)\n",
    "    - Place citations immediately after the relevant information\n",
    "    - Ensure each citation is correctly matched to its source\n",
    "    - Return only the plain text summary without markdown formatting\n",
    "    \"\"\"\n",
    "\n",
    "    formatted_context = \"\\n\".join(\n",
    "        f\"Content: {doc['content']}\\nSource: {doc['metadata']['name']}\\nPath: {doc['metadata']['path']}\"\n",
    "        for doc in context_documents\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Here are the documents to summarize:\n",
    "    \n",
    "    {formatted_context}\n",
    "    \n",
    "    Provide a concise summary with proper citations:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=llm_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    response_content = response[\"message\"][\"content\"]\n",
    "    \n",
    "    # Clean markdown formatting if present\n",
    "    try:\n",
    "        final_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "    except:\n",
    "        final_content = response_content.strip()\n",
    "\n",
    "    # Extract metadata from all documents\n",
    "    document_names = [doc['metadata']['name'] for doc in context_documents]\n",
    "    document_paths = [doc['metadata']['path'] for doc in context_documents]\n",
    "\n",
    "    return {\n",
    "        \"content\": final_content,\n",
    "        \"metadata\": {\n",
    "            \"name\": document_names,\n",
    "            \"path\": document_paths\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'DeepSeek R1 is a reasoning-focused large language model (LLM) developed with pure reinforcement learning, designed to excel in complex tasks like logic and mathematics while maintaining strong general capabilities. Retrieval-Augmented Generation (RAG) enhances LLM responses by incorporating relevant context before generating answers, ensuring more factual results. When using DeepSeek R1 for RAG applications, focus on leveraging its reasoning capabilities during the generation stage rather than employing it solely for retrieval tasks. [DeepSeek R1 Documentation](/data/documents/deepseek_r1_docs.pdf) and [RAG Best Practices Guide](/data/knowledge/rag_best_practices.md).',\n",
       " 'metadata': {'name': ['DeepSeek R1 Documentation',\n",
       "   'RAG Best Practices Guide'],\n",
       "  'path': ['/data/documents/deepseek_r1_docs.pdf',\n",
       "   '/data/knowledge/rag_best_practices.md']}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smry = source_summarizer(samples)\n",
    "smry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "DeepSeek R1 is a reasoning-focused large language model (LLM) developed with pure reinforcement learning, designed to excel in complex tasks like logic and mathematics while maintaining strong general capabilities. Retrieval-Augmented Generation (RAG) enhances LLM responses by incorporating relevant context before generating answers, ensuring more factual results. When using DeepSeek R1 for RAG applications, focus on leveraging its reasoning capabilities during the generation stage rather than employing it solely for retrieval tasks. [DeepSeek R1 Documentation](/data/documents/deepseek_r1_docs.pdf) and [RAG Best Practices Guide](/data/knowledge/rag_best_practices.md)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(smry['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "DeepSeek R1 is a reasoning-focused large language model that leverages reinforcement learning to develop advanced reasoning abilities. It excels in logic, mathematics, and reasoning benchmarks while maintaining strong general capabilities [DeepSeek R1 Documentation] (/data/documents/deepseek_r1_docs.pdf). In Retrieval-Augmented Generation (RAG) applications, DeepSeek R1's reasoning capabilities should be used for generation, rather than retrieval, to ground responses in factual information and reduce hallucinations [RAG Best Practices Guide] (/data/knowledge/rag_best_practices.md)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "smry2 = source_summarizer(samples, llm_model='llama3.2')\n",
    "display(Markdown(smry2['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lrrd-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
