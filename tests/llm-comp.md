Here's a comparison of key Ollama LLMs based on available data as of March 2025:

| Model             | Parameters | Storage Size | Context Window | Tool Use | Function Calling | Time consumed |
|-------------------|------------|--------------|----------------|----------|------------------|---------------|
| llama3.2          | 3B         | 2.0GB        | 130K tokens    | Y        | Y                | 2.5s          |
| llama3.3          | 70B        | 43GB         | 128K tokens    | Y        | Y                | 2.5s          |
| phi4-mini         | 3.8B       | 2.5GB        | 128K tokens    | Y        | Y                | 2.5s          |
| mistral-small     | 24B        | 14GB         | 128K tokens    | Y        | Y                | 2.5s          |
| deepseek-r1       | 7B         | 4.7GB        | 128K tokens    | Y        | Y*               | 2.5s          |
| deepseek-r1:70B   | 70B        | 43GB         | 128K tokens    | Y        | Y*               | 2.5s          |
| gemma3:27b        | 27B        | 17GB         | 128K tokens    | Y        | Y                | 2.5s          |
| qwq               | Unknown    | 20GB         | 131K tokens    | N        | N                | 2.5s          |          

*DeepSeek-R1 function calling capabilities are currently unstable according to official documentation